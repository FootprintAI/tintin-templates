{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOp(model_relative_path: str = 'model', model_name: str = 'resnet_graphdef', epochs: int = 50, networks: int = 3):\n",
    "    ### trainOp served as the main entry point (like main function for most of program languages)\n",
    "    ### it takes several mandatory parameters to configure the path of saved/generated model file\n",
    "    \n",
    "    ### model_relative_path: A persistent volume by default will be mounted at `/home/jovyan` (we called application root here),\n",
    "    ### to keep your state(i.e. files generated) in place.\n",
    "    ### And by default, we want to keep our model generated in `model` relative to application root.\n",
    "    ### However, in background job or hyperparameter tunning, this application root may CHANGED.\n",
    "    ### So in order to perserve model, we use `model_relative_path` to discard variant application root caused.\n",
    "    \n",
    "    ### model_name: a model name will be generated as a file holder (i.e. folder) to all model files generated.\n",
    "    ### In real scenario, the stucture of model file should depends on the inference infrastructure.\n",
    "    ### and in our integration, we requires model should be structured in below:\n",
    "    ### model/\n",
    "    ###        $model_name/\n",
    "    ###                    config.pbtxt\n",
    "    ###                    labels.txt\n",
    "    ###                    $version/\n",
    "    ###                             model.savedmodel/\n",
    "    ###                                              saved_models.pb\n",
    "    \n",
    "    import json\n",
    "    import sys\n",
    "    import os\n",
    "    import pathlib\n",
    "\n",
    "\n",
    "    home = '/home/jovyan'\n",
    "    \n",
    "   \n",
    "    prep_data_dir = os.path.join(home, \"dataprep\")\n",
    "    output_model_dir = os.path.join(home, model_relative_path)\n",
    "    temp_data_dir  = os.path.join(home, \".temp\")\n",
    "    model_version = os.environ.get('TINTIN_SESSION_TEMPLATE_MODEL_VERSION', '1')\n",
    "    \n",
    "    \n",
    "    print(\"outputdir:\", prep_data_dir)\n",
    "    pathlib.Path(prep_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(output_model_dir).mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(temp_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # This tranform_data will be used in online serving to tranform data in preprocessing\n",
    "    from tintin.online_serving import save_function_to_model\n",
    "    from tintin.online_serving import transform_data_file_name\n",
    "    @save_function_to_model(model_path=output_model_dir, file_name=transform_data_file_name)\n",
    "    def transform_data(input_data):\n",
    "        input_data = input_data.astype('float32') / 255\n",
    "        input_data -= 0.5\n",
    "        return input_data\n",
    "    \n",
    "    \n",
    "    def load_data():\n",
    "        \n",
    "        import numpy as np\n",
    "        from tensorflow.keras.datasets import cifar10\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "        \n",
    "        np.save(os.path.join(prep_data_dir, 'x_train.npy'), x_train)\n",
    "        np.save(os.path.join(prep_data_dir, 'y_train.npy'), y_train)\n",
    "        np.save(os.path.join(prep_data_dir, 'x_test.npy'), x_test)\n",
    "        np.save(os.path.join(prep_data_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "        \n",
    "    def train(epochs=50, networks=3):\n",
    "        \n",
    "        import os\n",
    "        import shutil\n",
    "        import argparse\n",
    "        import numpy as np\n",
    "\n",
    "        from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "        from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n",
    "        from tensorflow.python.saved_model import tag_constants\n",
    "        from tensorflow.python.saved_model import signature_constants\n",
    "\n",
    "        from tensorflow.keras.regularizers import l2\n",
    "        from tensorflow.keras.models import Model\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        from tensorflow.keras.models import load_model\n",
    "        from tensorflow.keras.models import save_model\n",
    "        from tensorflow.keras.layers import Dense, Conv2D\n",
    "        from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "        from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
    "        from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "        from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "        from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "        from tensorflow.keras import utils\n",
    "        from tensorflow.keras import layers\n",
    "        from tensorflow.keras import callbacks as keras_callbacks\n",
    "        #import tensorflow.python.compiler.tensorrt as trt\n",
    "\n",
    "        CONT_TRTIS_RESOURCE_DIR = 'trtis_resource'\n",
    "\n",
    "        # Copy TRTIS resource (containing config.pbtxt, labels.txt, ...) from container to mounted volume\n",
    "        model_dir = os.path.join(output_model_dir, model_name)\n",
    "        if model_version == '1': # if it is default version, we always clear it to keep the space clean\n",
    "            model_verison_dir = os.path.join(output_model_dir, model_name, model_version)\n",
    "            if os.path.isdir(model_verison_dir):\n",
    "                shutil.rmtree(model_verison_dir)\n",
    "        configdotpbtxt = \"\"\"name: \"resnet_graphdef\"\n",
    "platform: \"tensorflow_savedmodel\"\n",
    "max_batch_size: 128\n",
    "\n",
    "\n",
    "input [\n",
    "  {\n",
    "    name: \"input_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NHWC\n",
    "    dims: [ 32, 32, 3 ]\n",
    "  }\n",
    "]\n",
    "\n",
    "output [\n",
    "  {\n",
    "    name: \"dense\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 10 ]\n",
    "    label_filename: \"labels.txt\"\n",
    "  }\n",
    "]\n",
    "\n",
    "version_policy: { all { }}\n",
    "\"\"\"\n",
    "        labeldottxt = \"\"\"airplane\n",
    "automobile\n",
    "bird\n",
    "cat\n",
    "deer\n",
    "dog\n",
    "frog\n",
    "horse\n",
    "ship\n",
    "truck\"\"\"\n",
    "\n",
    "        \n",
    "        pathlib.Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "        with open(os.path.join(model_dir, 'config.pbtxt'), 'w') as f:\n",
    "            f.write(configdotpbtxt)\n",
    "        with open(os.path.join(model_dir, 'labels.txt'), 'w') as f:\n",
    "            f.write(labeldottxt)\n",
    "\n",
    "        # Training parameters\n",
    "        batch_size = 128  # orig paper trained all networks with batch_size=128\n",
    "        epochs = int(epochs)\n",
    "        data_augmentation = True\n",
    "        num_classes = 10\n",
    "\n",
    "        n = int(networks)\n",
    "        # Model version\n",
    "        # Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
    "        version = 2\n",
    "\n",
    "        # Computed depth from supplied model parameter n\n",
    "        if version == 1:\n",
    "            depth = n * 6 + 2\n",
    "        elif version == 2:\n",
    "            depth = n * 9 + 2\n",
    "\n",
    "        # Model name, depth and version\n",
    "        model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "        # Load the CIFAR10 data.\n",
    "        x_train = np.load(os.path.join(prep_data_dir, \"x_train.npy\"))\n",
    "        y_train = np.load(os.path.join(prep_data_dir, \"y_train.npy\"))\n",
    "        x_test = np.load(os.path.join(prep_data_dir, \"x_test.npy\"))\n",
    "        y_test = np.load(os.path.join(prep_data_dir, \"y_test.npy\"))\n",
    "\n",
    "        # Input image dimensions.\n",
    "        input_shape = x_train.shape[1:]\n",
    "\n",
    "        # Normalize data.\n",
    "        x_train = transform_data(x_train)\n",
    "        x_test = transform_data(x_test)\n",
    "\n",
    "        print('x_train shape:', x_train.shape)\n",
    "        print(x_train.shape[0], 'train samples')\n",
    "        print(x_test.shape[0], 'test samples')\n",
    "        print('y_train shape:', y_train.shape)\n",
    "\n",
    "        # Convert class vectors to binary class matrices.\n",
    "        y_train = utils.to_categorical(y_train, num_classes)\n",
    "        y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "        def lr_schedule(epoch):\n",
    "            \"\"\"Learning Rate Schedule\n",
    "\n",
    "            Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "            Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "            # Arguments\n",
    "                epoch (int): The number of epochs\n",
    "\n",
    "            # Returns\n",
    "                lr (float32): learning rate\n",
    "            \"\"\"\n",
    "            lr = 1e-3\n",
    "            if epoch > 180:\n",
    "                lr *= 0.5e-3\n",
    "            elif epoch > 160:\n",
    "                lr *= 1e-3\n",
    "            elif epoch > 120:\n",
    "                lr *= 1e-2\n",
    "            elif epoch > 80:\n",
    "                lr *= 1e-1\n",
    "            print('Learning rate: ', lr)\n",
    "            return lr\n",
    "\n",
    "        def resnet_layer(inputs,\n",
    "                         num_filters=16,\n",
    "                         kernel_size=3,\n",
    "                         strides=1,\n",
    "                         activation='relu',\n",
    "                         batch_normalization=True,\n",
    "                         conv_first=True):\n",
    "            \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "            # Arguments\n",
    "                inputs (tensor): input tensor from input image or previous layer\n",
    "                num_filters (int): Conv2D number of filters\n",
    "                kernel_size (int): Conv2D square kernel dimensions\n",
    "                strides (int): Conv2D square stride dimensions\n",
    "                activation (string): activation name\n",
    "                batch_normalization (bool): whether to include batch normalization\n",
    "                conv_first (bool): conv-bn-activation (True) or\n",
    "                    bn-activation-conv (False)\n",
    "\n",
    "            # Returns\n",
    "                x (tensor): tensor as input to the next layer\n",
    "            \"\"\"\n",
    "            conv = Conv2D(num_filters,\n",
    "                          kernel_size=kernel_size,\n",
    "                          strides=strides,\n",
    "                          padding='same',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          kernel_regularizer=l2(1e-4))\n",
    "\n",
    "            x = inputs\n",
    "            if conv_first:\n",
    "                x = conv(x)\n",
    "                if batch_normalization:\n",
    "                    x = BatchNormalization()(x)\n",
    "                if activation is not None:\n",
    "                    x = Activation(activation)(x)\n",
    "            else:\n",
    "                if batch_normalization:\n",
    "                    x = BatchNormalization()(x)\n",
    "                if activation is not None:\n",
    "                    x = Activation(activation)(x)\n",
    "                x = conv(x)\n",
    "            return x\n",
    "\n",
    "        def resnet_v1(input_shape, depth, num_classes=10):\n",
    "            \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "            Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "            Last ReLU is after the shortcut connection.\n",
    "            At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "            by a convolutional layer with strides=2, while the number of filters is\n",
    "            doubled. Within each stage, the layers have the same number filters and the\n",
    "            same number of filters.\n",
    "            Features maps sizes:\n",
    "            stage 0: 32x32, 16\n",
    "            stage 1: 16x16, 32\n",
    "            stage 2:  8x8,  64\n",
    "            The Number of parameters is approx the same as Table 6 of [a]:\n",
    "            ResNet20 0.27M\n",
    "            ResNet32 0.46M\n",
    "            ResNet44 0.66M\n",
    "            ResNet56 0.85M\n",
    "            ResNet110 1.7M\n",
    "\n",
    "            # Arguments\n",
    "                input_shape (tensor): shape of input image tensor\n",
    "                depth (int): number of core convolutional layers\n",
    "                num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "            # Returns\n",
    "                model (Model): Keras model instance\n",
    "            \"\"\"\n",
    "            if (depth - 2) % 6 != 0:\n",
    "                raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "            # Start model definition.\n",
    "            num_filters = 16\n",
    "            num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "            inputs = Input(shape=input_shape)\n",
    "            x = resnet_layer(inputs=inputs)\n",
    "            # Instantiate the stack of residual units\n",
    "            for stack in range(3):\n",
    "                for res_block in range(num_res_blocks):\n",
    "                    strides = 1\n",
    "                    if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                        strides = 2  # downsample\n",
    "                    y = resnet_layer(inputs=x,\n",
    "                                     num_filters=num_filters,\n",
    "                                     strides=strides)\n",
    "                    y = resnet_layer(inputs=y,\n",
    "                                     num_filters=num_filters,\n",
    "                                     activation=None)\n",
    "                    if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                        # linear projection residual shortcut connection to match\n",
    "                        # changed dims\n",
    "                        x = resnet_layer(inputs=x,\n",
    "                                         num_filters=num_filters,\n",
    "                                         kernel_size=1,\n",
    "                                         strides=strides,\n",
    "                                         activation=None,\n",
    "                                         batch_normalization=False)\n",
    "                    x = layers.add([x, y])\n",
    "                    x = Activation('relu')(x)\n",
    "                num_filters *= 2\n",
    "\n",
    "            # Add classifier on top.\n",
    "            # v1 does not use BN after last shortcut connection-ReLU\n",
    "            x = AveragePooling2D(pool_size=8)(x)\n",
    "            y = Flatten()(x)\n",
    "            outputs = Dense(num_classes,\n",
    "                            activation='softmax',\n",
    "                            kernel_initializer='he_normal')(y)\n",
    "\n",
    "            # Instantiate model.\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            return model\n",
    "\n",
    "        \n",
    "        class LossAndErrorPrintingCallback(keras_callbacks.Callback):\n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                print(\"epoch={}\".format(epoch))\n",
    "                print(\"Training-Accuracy={:7.6f}\".format(logs[\"accuracy\"]))\n",
    "                print(\"Training-Loss={:7.6f}\".format(logs[\"loss\"]))\n",
    "                print(\"Validation-Accuracy={:7.6f}\".format(logs[\"val_accuracy\"]))\n",
    "                print(\"Validation-Loss={:7.6f}\".format(logs[\"val_loss\"]))\n",
    "        \n",
    "        def resnet_v2(input_shape, depth, num_classes=10):\n",
    "            \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "            Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "            bottleneck layer\n",
    "            First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "            Second and onwards shortcut connection is identity.\n",
    "            At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "            by a convolutional layer with strides=2, while the number of filter maps is\n",
    "            doubled. Within each stage, the layers have the same number filters and the\n",
    "            same filter map sizes.\n",
    "            Features maps sizes:\n",
    "            conv1  : 32x32,  16\n",
    "            stage 0: 32x32,  64\n",
    "            stage 1: 16x16, 128\n",
    "            stage 2:  8x8,  256\n",
    "\n",
    "            # Arguments\n",
    "                input_shape (tensor): shape of input image tensor\n",
    "                depth (int): number of core convolutional layers\n",
    "                num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "            # Returns\n",
    "                model (Model): Keras model instance\n",
    "            \"\"\"\n",
    "            if (depth - 2) % 9 != 0:\n",
    "                raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "            # Start model definition.\n",
    "            num_filters_in = 16\n",
    "            num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "            inputs = Input(shape=input_shape)\n",
    "            # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "            x = resnet_layer(inputs=inputs,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=True)\n",
    "\n",
    "            # Instantiate the stack of residual units\n",
    "            for stage in range(3):\n",
    "                for res_block in range(num_res_blocks):\n",
    "                    activation = 'relu'\n",
    "                    batch_normalization = True\n",
    "                    strides = 1\n",
    "                    if stage == 0:\n",
    "                        num_filters_out = num_filters_in * 4\n",
    "                        if res_block == 0:  # first layer and first stage\n",
    "                            activation = None\n",
    "                            batch_normalization = False\n",
    "                    else:\n",
    "                        num_filters_out = num_filters_in * 2\n",
    "                        if res_block == 0:  # first layer but not first stage\n",
    "                            strides = 2    # downsample\n",
    "\n",
    "                    # bottleneck residual unit\n",
    "                    y = resnet_layer(inputs=x,\n",
    "                                     num_filters=num_filters_in,\n",
    "                                     kernel_size=1,\n",
    "                                     strides=strides,\n",
    "                                     activation=activation,\n",
    "                                     batch_normalization=batch_normalization,\n",
    "                                     conv_first=False)\n",
    "                    y = resnet_layer(inputs=y,\n",
    "                                     num_filters=num_filters_in,\n",
    "                                     conv_first=False)\n",
    "                    y = resnet_layer(inputs=y,\n",
    "                                     num_filters=num_filters_out,\n",
    "                                     kernel_size=1,\n",
    "                                     conv_first=False)\n",
    "                    if res_block == 0:\n",
    "                        # linear projection residual shortcut connection to match\n",
    "                        # changed dims\n",
    "                        x = resnet_layer(inputs=x,\n",
    "                                         num_filters=num_filters_out,\n",
    "                                         kernel_size=1,\n",
    "                                         strides=strides,\n",
    "                                         activation=None,\n",
    "                                         batch_normalization=False)\n",
    "                    x = layers.add([x, y])\n",
    "\n",
    "                num_filters_in = num_filters_out\n",
    "\n",
    "            # Add classifier on top.\n",
    "            # v2 has BN-ReLU before Pooling\n",
    "            x = BatchNormalization()(x)\n",
    "            x = Activation('relu')(x)\n",
    "            x = AveragePooling2D(pool_size=8)(x)\n",
    "            y = Flatten()(x)\n",
    "            outputs = Dense(num_classes,\n",
    "                            activation='softmax',\n",
    "                            kernel_initializer='he_normal')(y)\n",
    "\n",
    "            # Instantiate model.\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            return model\n",
    "\n",
    "        if version == 2:\n",
    "            model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "        else:\n",
    "            model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(lr=lr_schedule(0)),\n",
    "                      metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        print(model_type)\n",
    "\n",
    "        # Prepare model model saving directory.\n",
    "        \n",
    "        save_dir = os.path.join(home, 'saved_models')\n",
    "        save_model_name = 'cifar10_%s_model.%03d.h5' % (model_type, epochs)\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        filepath = os.path.join(save_dir, save_model_name)\n",
    "\n",
    "        # Prepare callbacks for model saving and for learning rate adjustment.\n",
    "        checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                                     monitor='val_accuracy',\n",
    "                                     verbose=1,\n",
    "                                     save_best_only=True)\n",
    "\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "        lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                                       cooldown=0,\n",
    "                                       patience=5,\n",
    "                                       min_lr=0.5e-6)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        callbacks = [checkpoint, LossAndErrorPrintingCallback(), lr_reducer, lr_scheduler]\n",
    "\n",
    "        # Run training, with or without data augmentation.\n",
    "        if not data_augmentation:\n",
    "            print('Not using data augmentation.')\n",
    "            model.fit(x_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      validation_data=(x_test, y_test),\n",
    "                      shuffle=True,\n",
    "                      verbose=2,\n",
    "                      callbacks=callbacks)\n",
    "        else:\n",
    "            print('Using real-time data augmentation.')\n",
    "            # This will do preprocessing and realtime data augmentation:\n",
    "            datagen = ImageDataGenerator(\n",
    "                # set input mean to 0 over the dataset\n",
    "                featurewise_center=False,\n",
    "                # set each sample mean to 0\n",
    "                samplewise_center=False,\n",
    "                # divide inputs by std of dataset\n",
    "                featurewise_std_normalization=False,\n",
    "                # divide each input by its std\n",
    "                samplewise_std_normalization=False,\n",
    "                # apply ZCA whitening\n",
    "                zca_whitening=False,\n",
    "                # epsilon for ZCA whitening\n",
    "                zca_epsilon=1e-06,\n",
    "                # randomly rotate images in the range (deg 0 to 180)\n",
    "                rotation_range=0,\n",
    "                # randomly shift images horizontally\n",
    "                width_shift_range=0.1,\n",
    "                # randomly shift images vertically\n",
    "                height_shift_range=0.1,\n",
    "                # set range for random shear\n",
    "                shear_range=0.,\n",
    "                # set range for random zoom\n",
    "                zoom_range=0.,\n",
    "                # set range for random channel shifts\n",
    "                channel_shift_range=0.,\n",
    "                # set mode for filling points outside the input boundaries\n",
    "                fill_mode='nearest',\n",
    "                # value used for fill_mode = \"constant\"\n",
    "                cval=0.,\n",
    "                # randomly flip images\n",
    "                horizontal_flip=True,\n",
    "                # randomly flip images\n",
    "                vertical_flip=False,\n",
    "                # set rescaling factor (applied before any other transformation)\n",
    "                rescale=None,\n",
    "                # set function that will be applied on each input\n",
    "                preprocessing_function=None,\n",
    "                # image data format, either \"channels_first\" or \"channels_last\"\n",
    "                data_format=None,\n",
    "                # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "                validation_split=0.0)\n",
    "\n",
    "            # Compute quantities required for featurewise normalization\n",
    "            # (std, mean, and principal components if ZCA whitening is applied).\n",
    "            datagen.fit(x_train)\n",
    "\n",
    "            # Fit the model on the batches generated by datagen.flow().\n",
    "            model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                steps_per_epoch=len(x_train)/batch_size,\n",
    "                                validation_data=(x_test, y_test),\n",
    "                                epochs=epochs, verbose=2, workers=1,\n",
    "                                use_multiprocessing=False,\n",
    "                                callbacks=callbacks)\n",
    "\n",
    "        # Score trained model.\n",
    "        \n",
    "        scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "        \n",
    "        #validAcc = scores[1]\n",
    "        #validLoss = scores[0]\n",
    "\n",
    "        # Save Keras model\n",
    "        tmp_model_path = os.path.join(temp_data_dir, \"tmp\")\n",
    "        if os.path.isdir(tmp_model_path):\n",
    "            shutil.rmtree(tmp_model_path)\n",
    "        os.mkdir(tmp_model_path)\n",
    "        \n",
    "\n",
    "        keras_model_path = os.path.join(tmp_model_path, 'keras_model.h5')\n",
    "        model.save(keras_model_path)\n",
    "\n",
    "        # Convert Keras model to Tensorflow SavedModel\n",
    "        def export_h5_to_pb(path_to_h5, export_path):\n",
    "             # Load the Keras model\n",
    "            keras_model = load_model(path_to_h5)\n",
    "            \n",
    "            save_model(model=keras_model, filepath=export_path, overwrite=True, include_optimizer=True, save_format=None, signatures=None, options=None, save_traces=True)\n",
    "\n",
    "        tf_model_path  = os.path.join(model_dir, model_version, 'model.savedmodel')\n",
    "        export_h5_to_pb(keras_model_path, tf_model_path)\n",
    "        # Remove tmp dirs\n",
    "        shutil.rmtree(tmp_model_path)\n",
    "        return\n",
    "    \n",
    "    child_pid = os.fork()\n",
    "    if child_pid == 0:\n",
    "        # child process get 0 pid\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        \n",
    "        load_data()\n",
    "        \n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        # parent process get child pid\n",
    "        os.waitpid(child_pid, 0)\n",
    "    \n",
    "    train(epochs, networks)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### if you want to debug the above tranOp function,\n",
    "### uncomment below\n",
    "### and remember to COMMENT it before you `BUILD` this pipeline through UI\n",
    "\n",
    "# = trainOp('model', 'resnet_graphdef', 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"requirements.txt\", \"w\") as f:\n",
    "    f.write(\"kfp==0.5.1\\n\")\n",
    "    f.write(\"h5py<3.0.0\\n\")\n",
    "    f.write(\"keras==2.3.1\\n\")\n",
    "    f.write(\"tintin-sdk>=0.0.4\\n\")\n",
    "\n",
    "!pip install -r requirements.txt --user --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp\n",
    "import kfp.compiler as compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pvcname = os.environ.get('TINTIN_SESSION_TEMPLATE_PVC_NAME')\n",
    "generated_pipeline_zip_filename = os.environ.get('TINTIN_SESSION_TEMPLATE_GENERATED_PIPELINE_ZIP_FILENAME')\n",
    "gpu_type_list_text = os.environ.get('TINTIN_SESSION_TEMPLATE_GPU_TYPE_LIST')\n",
    "default_image = os.environ.get('TINTIN_SESSION_TEMPLATE_DEFAULT_IMAGE', 'footprintai/nvidia-tensorflow:19.12-tf1-py3')\n",
    "mountPath = os.environ.get('TINTIN_SESSION_TEMPLATE_MOUNT_PATH', '/home/jovyan')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainComp = comp.func_to_container_op(trainOp, \n",
    "                                      base_image=default_image,\n",
    "                                      packages_to_install=[\"keras==2.3.1\", \"h5py<3.0.0\", \"tintin-sdk>=0.0.4\"])\n",
    "\n",
    "import kfp.dsl as dsl\n",
    "@dsl.pipeline(\n",
    "   name='Projectname pipeline',\n",
    "   description='simple pipeline.'\n",
    ")\n",
    "def templated_pipeline_func(\n",
    "    epochs=50,\n",
    "    networks=3,\n",
    "):\n",
    "    \n",
    "    ### model relative path can NOT be nest path(e.g. a/b/c/d, it should be the first folder (e.g. model)\n",
    "    model_relative_path = os.environ.get('TINTIN_SESSION_TEMPLATE_MODEL_RELATIVE_PATH', 'model')    \n",
    "    model_name = os.environ.get('TINTIN_SESSION_TEMPLATE_MODEL_NAME', 'resnet_graphdef')\n",
    "    ### if you want to customize $model_name, replace `my_customized_model_name` and uncomment below\n",
    "    ### model_name = os.environ.get('TINTIN_SESSION_TEMPLATE_MODEL_NAME', 'my_customized_model_name')\n",
    "    \n",
    "    train_task = trainComp(model_relative_path, model_name, epochs, networks)\n",
    "    # add train_task default resources for cpu and memory, this value will be changed during runtime\n",
    "    # to reflect your settings in UI\n",
    "    train_task = train_task.add_resource_request('cpu', '1')\n",
    "    train_task = train_task.add_resource_limit('cpu', '1')\n",
    "    train_task = train_task.add_resource_request('memory', '4Gi')\n",
    "    train_task = train_task.add_resource_limit('memory', '4Gi')\n",
    "    \n",
    "    # add annotation to reflect our configuration on `model_relative_path` and `model_name` to workflow itself.\n",
    "    train_task = train_task.add_pod_annotation('tintin.footprint-ai.com/session-model-relative-path', model_relative_path)    \n",
    "    train_task = train_task.add_pod_annotation('tintin.footprint-ai.com/session-model-name', model_name)\n",
    "compiler.Compiler().compile(templated_pipeline_func, generated_pipeline_zip_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
